{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Apache Spark\n",
    "Based on readings of the book, \"Learning Spark : Lightning-Fast Big Data Analysis\", by Karau, Holden, et al. , O'Reilly Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Stack\n",
    "![Spark stacj](SparkStack.png)\n",
    "__Spark Core__ contains the basic functionality of Spark, including components for task\n",
    "scheduling,  memory  management,  fault  recovery,  interacting  with  storage  systems,\n",
    "and more. Spark Core is also home to the API that defines resilient distributed data‐\n",
    "sets  (RDDs),  which  are  Spark’s  main  programming  abstraction.\n",
    "\n",
    "__Cluster Managers__ Spark is designed to efficiently scale up from one to many thousands of compute nodes. To achieve this while maximizing flexibility, Spark can run over a variety of cluster managers (e.g., Hadoop YARN, Apache Mesos). A simple\n",
    "cluster manager is included in Spark called the Standalone Scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark: Master-Slave](ApacheSpark.png)\n",
    "The `SparkContext` \n",
    "connects to the Spark cluster manager, which then allocates resources across the \n",
    "worker nodes for the application. The cluster manager allocates executors across  \n",
    "the cluster worker nodes. It copies the application jar file to the workers, and finally \n",
    "it allocates tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Basics\n",
    "An RDD (__R__ esilient __D__ istributed __D__ ataset) is simply an immutable distributed collection of objects. In\n",
    "Spark  all  work  is  expressed  as one of these three steps:\n",
    "1. creating  new  RDDs \n",
    "2. transforming  existing RDDs (__Transformations__) \n",
    "3. calling operations (__Actions__) on RDDs to compute a result. \n",
    "\n",
    "Under the hood, Spark automatically distributes the data contained in RDDs across a cluster and parallelizes the operations one performs on them. Each RDD is split into multiple partitions, which may be stored (and manipulated) on different nodes of the cluster. RDDs can contain any type of Python object, including user-defined classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations vs. Actions\n",
    "Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, Spark computes them only in a lazy fashion - that is, the first time they are used in an action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init('C:\\\\Users\\\\ry2333\\\\spark-hadoop')\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "inputRDD = sc.textFile(\"log.txt\")\n",
    "## Transformations - All create new RDDs\n",
    "# Get errors\n",
    "errorsRDD = inputRDD.filter(lambda x: \"Error\" in x)\n",
    "# Get Warnings\n",
    "warningsRDD = inputRDD.filter(lambda x: \"Warning\" in x)\n",
    "# Combine them (union)\n",
    "badLinesRDD = errorsRDD.union(warningsRDD)\n",
    "\n",
    "## Lets do some actions \n",
    "# action 1: count\n",
    "print(\"Input had \", badLinesRDD.count(), \" concerning lines\")\n",
    "print(\"Here are 2 examples:\")\n",
    "# action 2: take (gets 2 lines)\n",
    "for line in badLinesRDD.take(2):\n",
    "    print(line)\n",
    "print(\"Here is the complete dump:\")\n",
    "# action 3: collect (gets all lines)\n",
    "for line in badLinesRDD.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another way to create an RDD\n",
    "The simplest way to create RDDs is to take an existing collection in your program and  pass  it  to  SparkContext’s  `parallelize()`   method. This approach is very useful when you are learning Spark, since you can quickly create your own RDDs in the shell and perform operations on them. Outside of prototyping and testing, this is not widely used since it requires that you have your entire dataset in memory on one machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Transformations\n",
    "\n",
    "### Element-wise transformations\n",
    "### `map()` vs `filter()`\n",
    "\n",
    "`map()` Takes a function and performs it on each element\n",
    "`filter()` Takes a function and returns an RDD that only has elements that pass the filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "#map example\n",
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "squared = nums.map(lambda x: x * x).collect()\n",
    "for num in squared:\n",
    "    print (\"%i \" % (num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "2 \n",
      "4 \n"
     ]
    }
   ],
   "source": [
    "#filter example\n",
    "nums = sc.parallelize([1, 2, 3, 4])\n",
    "filteredNums = nums.filter(lambda x: x !=3).collect()\n",
    "for num in filteredNums:\n",
    "    print (\"%i \" % (num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map` vs `flatMap`\n",
    "You  can think of  `flatMap()`  as “flattening” the iterators returned to it, so that instead of ending up with an RDD of lists we have an RDD of the elements in those lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['coffee', 'panda'], ['happy', 'panda'], ['happiest', 'panda', 'party']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([\"coffee panda\", \"happy panda\", \"happiest panda party\"])\n",
    "mappedRDD = data.map(lambda line: line.split(\" \"))\n",
    "flatMappedRDD = data.flatMap(lambda line: line.split(\" \"))\n",
    "mappedRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coffee', 'panda', 'happy', 'panda', 'happiest', 'panda', 'party']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatMappedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Set Operations\n",
    "These are still transformations\n",
    " 1. Distinct\n",
    " 2. Union\n",
    " 3. Intersection\n",
    " 4. Subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['panda', 'coffee', 'tea', 'monkey']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([\"coffee\", \"coffee\", \"panda\", \"monkey\", \"tea\"])\n",
    "rdd2 = sc.parallelize([\"coffee\", \"monkey\", \"kitty\"])\n",
    "distinctRDD = rdd1.distinct()\n",
    "distinctRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coffee', 'coffee', 'panda', 'monkey', 'tea', 'coffee', 'monkey', 'kitty']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unionRDD = rdd1.union(rdd2)\n",
    "unionRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coffee', 'monkey']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersectionRDD = rdd1.intersection(rdd2)\n",
    "intersectionRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['panda', 'tea']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtractRDD = rdd1.subtract(rdd2)\n",
    "subtractRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common  Actions\n",
    "`reduce()` is the most action, which takes a function that operates on two elements of the type in your RDD and returns a new element of the same type (`reduce(func)`)\n",
    "\n",
    "Note that reduce() requires that the return type of our result be the same type as that of the elements in the RDD we are operating over.\n",
    "\n",
    "The  `aggregate()`  function frees us from the constraint of having the return be the same type as the RDD we are working on. (`aggregate(zeroValue) (seqOp, combOp)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "sum = nums.reduce(lambda x, y: (x+y))\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    }
   ],
   "source": [
    "sumCount = nums.aggregate((0, 0),\n",
    "               (lambda acc, value: (acc[0] + value, acc[1] + 1)),\n",
    "               (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))\n",
    "print(sumCount[0] / float(sumCount[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Key-Value pairs\n",
    "Spark provides special operations on RDDs containing key-value pairs. These RDDs are called pair RDDs.  Pair RDDs are a useful building block in many programs, as they expose operations that allow you to act on each key in parallel or regroup data across the network. For example, pair RDDs have a  `reduceByKey()`  method that can\n",
    "aggregate  data  separately  for  each  key,  and  a  `join()`   method  that  can  merge  two RDDs  together  by  grouping  elements  with  the  same  key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairsRDD = sc.parallelize({(1, 2), (3, 4), (3, 6)})\n",
    "pairsRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine values with the same key.\n",
    "reduceByRDD = pairsRDD.reduceByKey(lambda x, y: x + y)\n",
    "reduceByRDD.collect() # [(1, 2), (3, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x264cc56dda0>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x264cc56dcf8>)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group values with the same key.\n",
    "groupByRDD = pairsRDD.groupByKey()\n",
    "groupByRDD.collect() # [(1, [2]), (3, [4, 6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (3, 5), (3, 7)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a function to each value of a pair RDD without changing the key.\n",
    "mapValuesRDD = pairsRDD.mapValues(lambda x: x + 1)\n",
    "mapValuesRDD.collect() # [(1, 3), (3, 5), (3, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (1, 4), (1, 5), (3, 4), (3, 5)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply a function that returns an iterator to each value of \n",
    "# a pair RDD, and for each element returned, produce a key/value\n",
    "# entry with the old key. Often used for tokenization.\n",
    "flatmapValuesRDD = pairsRDD.flatMapValues(lambda x: range(x,6))\n",
    "flatmapValuesRDD.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys only RDD\n",
    "keysRDD = pairsRDD.keys() # keysRDD has [1, 3, 3]\n",
    "# values only RDD\n",
    "valuesRDD = pairsRDD.values() # valuesRDD has [2, 4, 6]\n",
    "valuesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (3, (10, 2))]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per-key average (sum and count) with reduceByKey() and mapValues() in Python\n",
    "perKeyAvg = pairsRDD.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "perKeyAvg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on two pair RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otherPairsRDD = sc.parallelize({(3, 9)})\n",
    "# Remove elements with a key present in the other RDD.\n",
    "diffRDD = pairsRDD.subtractByKey(otherPairsRDD)\n",
    "diffRDD.collect() # [(1, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join\n",
    "Some  of  the  most  useful  operations  we  get  with  keyed  data  comes  from  using  it together  with  other  keyed  data.  Joining  data  together  is  probably  one  of  the  most common operations on a pair RDD. \n",
    "\n",
    "Only keys that are present in __both__ pair RDDs  are  output.  When  there  are  multiple  values  for  the  same key  in  one  of  the\n",
    "inputs,  the  resulting  pair  RDD  will  have  an  entry  for  every  possible  pair  of  values with that key from the two input RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (4, 9)), (3, (6, 9))]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform an inner join between two RDDs.\n",
    "innerJoinRDD = pairsRDD.join(otherPairsRDD)\n",
    "innerJoinRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to lift the restriction that __both__ pair RDDS must have the key we can use a `leftOuterJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, None)), (3, (4, 9)), (3, (6, 9))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a left Outer join between two RDDs.\n",
    "leftOuterJoinRDD = pairsRDD.leftOuterJoin(otherPairsRDD)\n",
    "leftOuterJoinRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"to be or not to be\".split()\n",
    "rdd = sc.parallelize(text)\n",
    "result = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
